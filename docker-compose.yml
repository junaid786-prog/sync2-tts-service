# Sync2 TTS Service - Docker Compose
# Supports Kokoro, Piper, XTTS v2, Fish Speech, Chatterbox-Turbo, CSM-1B TTS engines, and Moshi S2S
#
# Usage:
#   Kokoro:      docker-compose up kokoro-tts -d --build
#   Piper:       docker-compose up piper-tts -d --build
#   XTTS:        docker-compose up xtts-tts -d --build (requires GPU)
#   Fish Speech: docker-compose up fish-tts -d --build (requires GPU, most natural)
#   Chatterbox:  docker-compose up chatterbox-tts -d --build (requires GPU, sub-200ms)
#   CSM-1B:      docker-compose up csm-tts -d --build (requires GPU A10G+, conversational)
#   Moshi S2S:   docker-compose up moshi-s2s -d --build (requires GPU, real-time duplex)
#
# Switch between them:
#   TTS services use port 8765
#   Moshi S2S uses port 8770
#   docker stop sync2-tts-service && docker start sync2-piper-service
#   docker stop sync2-piper-service && docker start sync2-xtts-service
#   docker stop sync2-xtts-service && docker start sync2-fish-service
#   docker stop sync2-fish-service && docker start sync2-chatterbox-service
#   docker stop sync2-chatterbox-service && docker start sync2-csm-service

services:
  # Chatterbox-Turbo TTS (GPU - sub-200ms latency, highly expressive)
  chatterbox-tts:
    build:
      context: .
      dockerfile: Dockerfile.chatterbox
    container_name: sync2-chatterbox-service
    ports:
      - "8765:8765"
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - chatterbox-models:/app/models
      - ./voices:/app/voices
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8765/health"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 180s

  # Kokoro TTS (CPU - slower but expressive)
  kokoro-tts:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: sync2-tts-service
    ports:
      - "8765:8765"
    environment:
      - TTS_HOST=0.0.0.0
      - TTS_PORT=8765
      - TTS_MODEL=kokoro
      - TTS_DEFAULT_VOICE=af_sarah
      - TTS_USE_GPU=false
      - TTS_LOG_LEVEL=INFO
      - TTS_CACHE_ENABLED=true
    volumes:
      - tts-models:/app/models
      - tts-cache:/app/cache
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8765/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # Piper TTS (CPU - fast, lightweight)
  piper-tts:
    build:
      context: .
      dockerfile: Dockerfile.piper
    container_name: sync2-piper-service
    ports:
      - "8765:8765"
    environment:
      - TTS_HOST=0.0.0.0
      - TTS_PORT=8765
      - PIPER_BINARY=/app/piper/piper
      - PIPER_MODEL_PATH=/app/models/en_US-amy-medium.onnx
      - PIPER_CONFIG_PATH=/app/models/en_US-amy-medium.onnx.json
    volumes:
      - piper-models:/app/models
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8765/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # XTTS v2 TTS (GPU - human-like voice quality)
  xtts-tts:
    build:
      context: .
      dockerfile: Dockerfile.xtts
    container_name: sync2-xtts-service
    ports:
      - "8765:8765"
    environment:
      - TTS_HOST=0.0.0.0
      - TTS_PORT=8765
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - xtts-models:/app/models
      - xtts-cache:/root/.local/share/tts
      - ./voices:/app/voices
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8765/health"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 180s

  # Fish Speech TTS (GPU - most natural, human-like)
  fish-tts:
    build:
      context: .
      dockerfile: Dockerfile.fish
    container_name: sync2-fish-service
    ports:
      - "8765:8765"
    environment:
      - TTS_HOST=0.0.0.0
      - TTS_PORT=8765
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - fish-models:/app/models
      - fish-cache:/app/cache
      - ./voices:/app/voices
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8765/health"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 180s

  # CSM-1B TTS (GPU A10G+ - conversational, natural speech)
  csm-tts:
    build:
      context: .
      dockerfile: Dockerfile.csm
    container_name: sync2-csm-service
    ports:
      - "8765:8765"
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - NVIDIA_VISIBLE_DEVICES=all
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    volumes:
      - csm-models:/root/.cache/huggingface
      - ./voices:/app/voices
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import websockets; import asyncio; asyncio.run(websockets.connect('ws://localhost:8765/health'))"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 300s

  # Moshi S2S (GPU - real-time duplex speech-to-speech, ~200ms latency)
  moshi-s2s:
    build:
      context: .
      dockerfile: Dockerfile.moshi
    container_name: sync2-moshi-service
    ports:
      - "8770:8770"
    environment:
      - PYTHONUNBUFFERED=1
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - moshi-models:/root/.cache/huggingface
      - ./moshi_server/logs:/app/logs
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          memory: 4G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8770/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s

volumes:
  tts-models:
  tts-cache:
  piper-models:
  xtts-models:
  xtts-cache:
  fish-models:
  fish-cache:
  chatterbox-models:
  csm-models:
  moshi-models:
